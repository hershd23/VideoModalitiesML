{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, r2_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device(0)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Loader(data.Dataset):\n",
    "    def __init__(self, vid_path, aud_path, ids, labels, vid_transform=None, aud_transform=None):\n",
    "        self.vid_path = vid_path\n",
    "        self.aud_path = aud_path\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "        self.vid_transform = vid_transform        \n",
    "        self.aud_transform = aud_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def read_vid_data(self, path, selected_folder, use_transform):\n",
    "        X = []\n",
    "        for i in range(0,15):        \n",
    "            image = Image.open(path+selected_folder+\"/frame\"+str(i+1)+\".jpg\")\n",
    "        \n",
    "            if use_transform is not None:\n",
    "                image = use_transform(image)\n",
    "\n",
    "            X.append(image.squeeze_(0))\n",
    "        X = torch.stack(X, dim=0)\n",
    "        X = X.permute(1,0,2,3)\n",
    "        return X\n",
    "    \n",
    "    def read_aud_data(self, path, selected_file, use_transform):\n",
    "        \n",
    "        selected_file = selected_file.replace(\".\",\"\") + \".png\"\n",
    "        img_file = Path(path, selected_file)\n",
    "        image = Image.open(img_file).convert(\"RGB\")\n",
    "        \n",
    "        if use_transform is not None:\n",
    "            image = use_transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X_3d = self.read_vid_data(self.vid_path, self.ids[index][:-4], self.vid_transform)\n",
    "        X = self.read_aud_data(self.aud_path, self.ids[index][:-4], self.aud_transform)\n",
    "        y = torch.FloatTensor(self.labels[index])\n",
    "\n",
    "        return X_3d, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalityModel(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(MultiModalityModel, self).__init__()\n",
    "        \n",
    "        self.VideoResNet = models.video.r3d_18(pretrained)\n",
    "        self.VideoResNet.fc = nn.Linear(512, 512)\n",
    "        \n",
    "        self.AudioVgg = models.vgg11(pretrained=pretrained)\n",
    "        modules = list(self.AudioVgg.classifier.children())[:3]\n",
    "        modules.extend([nn.Linear(4096, 512)]) \n",
    "        self.AudioVgg.classifier = nn.Sequential(*modules)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                    nn.Linear(1024, 128),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Linear(128, 32),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Linear(32, 6),\n",
    "                    nn.Sigmoid()\n",
    "                    )\n",
    "        \n",
    "    def forward(self, x_3d, x):\n",
    "        \n",
    "        vid_feat = self.VideoResNet(x_3d) \n",
    "        aud_feat = self.AudioVgg(x)\n",
    "        x = torch.cat((vid_feat, aud_feat), 1)\n",
    "        x = self.fc(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Function\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    N_count = 0 \n",
    "    for batch_idx, (X_3D,X, y) in enumerate(train_loader):\n",
    "        X_3D, X, y = X_3D.to(device), X.to(device), y.to(device)\n",
    "        \n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_3D,X) \n",
    "        \n",
    "        criterion = nn.MSELoss(reduction = 'sum')\n",
    "        loss = criterion(output, y)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "\n",
    "    return losses\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluation(model, device, loader):\n",
    "    model.eval()\n",
    "    \n",
    "    loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    score = []\n",
    "    with torch.no_grad():\n",
    "        for (X_3D, X, y) in loader:\n",
    "            X_3D, X, y = X_3D.to(device), X.to(device), y.to(device)\n",
    "\n",
    "            output = model(X_3D, X)\n",
    "            \n",
    "            criterion = nn.MSELoss(reduction = 'sum')\n",
    "            loss = criterion(output, y)\n",
    "            loss += loss.item() \n",
    "            \n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y.cpu().detach().numpy())\n",
    "            all_y_pred.extend(output.cpu().detach().numpy())\n",
    "\n",
    "    loss /= len(loader.dataset)\n",
    "    \n",
    "    all_y = np.asarray(all_y)\n",
    "    all_y_pred = np.asarray(all_y_pred)\n",
    "    \n",
    "    for i in range(all_y.shape[1]):\n",
    "        score.extend([1 - mean_absolute_error(all_y[:,i], all_y_pred[:,i])])\n",
    "\n",
    "    return loss.cpu().detach().numpy(), np.asarray(score), all_y, all_y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Path\n",
    "training_aud_data_path = \"../audio/spectrogram/training_data/\"    \n",
    "validation_aud_data_path = \"../audio/spectrogram/validation_data/\"\n",
    "test_aud_data_path = \"../audio/spectrogram/test_data/\"\n",
    "\n",
    "training_vid_data_path = \"/home/ramsub/first-impressions/data/image_data/training_data/\"    \n",
    "validation_vid_data_path = \"/home/ramsub/first-impressions/data/image_data/validation_data/\"\n",
    "test_vid_data_path = \"/home/ramsub/first-impressions/data/image_data/test_data/\"\n",
    "\n",
    "save_model_path = \"./saved_models/\"\n",
    "\n",
    "#Read CSV files\n",
    "aud_train = pd.read_csv('../audio/pickle_files/training_df_all.csv')\n",
    "aud_test = pd.read_csv('../audio/pickle_files/test_df_all.csv')\n",
    "aud_val = pd.read_csv('../audio/pickle_files/validation_df_all.csv')\n",
    "\n",
    "labels = ['interview_score', 'openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
    "\n",
    "train_label = aud_train[labels].values\n",
    "train_list = aud_train['video_id'].values\n",
    "\n",
    "test_label = aud_test[labels].values\n",
    "test_list = aud_test['video_id'].values\n",
    "\n",
    "val_label = aud_val[labels].values\n",
    "val_list = aud_val['video_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the CNN Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "img_size_vid = 112\n",
    "img_size_aud = 224\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "l_decay = 5e-4\n",
    "log_interval = 1\n",
    "\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "print(device)\n",
    "\n",
    "train_params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "test_params = {'batch_size': batch_size, 'shuffle': False, 'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "vid_transform = transforms.Compose([transforms.Resize([img_size_vid, img_size_vid]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
    "                               ])\n",
    "\n",
    "aud_transform = transforms.Compose([transforms.Resize([img_size_aud, img_size_aud]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Dataset_Loader(\n",
    "    training_vid_data_path,\n",
    "    training_aud_data_path,\n",
    "    train_list, train_label,\n",
    "    vid_transform = vid_transform,\n",
    "    aud_transform = aud_transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_set, **train_params)\n",
    "\n",
    "val_set = Dataset_Loader(\n",
    "    validation_vid_data_path,\n",
    "    validation_aud_data_path,\n",
    "    val_list, val_label,\n",
    "    vid_transform = vid_transform,\n",
    "    aud_transform = aud_transform)\n",
    "\n",
    "val_loader = data.DataLoader(val_set, **test_params)\n",
    "\n",
    "test_set = Dataset_Loader(\n",
    "    test_vid_data_path,\n",
    "    test_aud_data_path,\n",
    "    test_list, test_label,\n",
    "    vid_transform = vid_transform,\n",
    "    aud_transform = aud_transform)\n",
    "\n",
    "test_loader = data.DataLoader(test_set, **test_params)\n",
    "\n",
    "model = MultiModalityModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "test_losses = []\n",
    "test_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(model, device, train_loader, optimizer, epoch, log_interval)    \n",
    "    \n",
    "    epoch_val_loss, epoch_val_score, test_annot, test_val_pred = evaluation(model, device, val_loader)\n",
    "    \n",
    "    val_losses.append(epoch_val_loss)    \n",
    "    val_scores.append(epoch_val_score)\n",
    "    \n",
    "    if min(val_losses) == epoch_val_loss:\n",
    "        torch.save(model.state_dict(), os.path.join(save_model_path, 'multi_model_trained.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing loss: 0.0011741\n",
      " Interview: 0.9180\n",
      " openness: 0.9102\n",
      " conscientiousness: 0.9153\n",
      " extraversion: 0.9150\n",
      " agreeableness: 0.9111\n",
      " neuroticism: 0.9100\n"
     ]
    }
   ],
   "source": [
    "saved_model = MultiModalityModel().to(device)\n",
    "saved_model.load_state_dict(torch.load(os.path.join(save_model_path, 'multi_model_trained.pth')))\n",
    "\n",
    "test_loss, test_score, test_annot, test_annot_pred = evaluation(saved_model, device, test_loader)\n",
    "print('\\nTesting loss: {:.7f}\\n Interview: {:.4f}\\n openness: {:.4f}\\n conscientiousness: {:.4f}\\n extraversion: {:.4f}\\n agreeableness: {:.4f}\\n neuroticism: {:.4f}'.format(test_loss, test_score[0],test_score[1],test_score[2],test_score[3],test_score[4],test_score[5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
